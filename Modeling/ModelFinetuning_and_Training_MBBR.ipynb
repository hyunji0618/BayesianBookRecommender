{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-Wv5y6J-tUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwL-A3ao-uc1",
        "outputId": "f546a39d-a56b-4e99-eef5-bb6a568cb2d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score\n",
        "!pip install openpyxl\n",
        "# Load dataset\n",
        "#df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/book_ratings_cleaned.xlsx\")\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/book_ratings_cleaned.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmALFuOIlVRO",
        "outputId": "e7f7ad02-ea19-4ffd-8130-f548f46ccebd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSlGMdzbI4EI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHVtN6uGJGcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "print(\"Is GPU available?\", torch.cuda.is_available())\n",
        "!pip install pymc[jax] --quiet\n",
        "!pip install jax jaxlib --quiet\n",
        "\n",
        "\n",
        "def evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\n",
        "    mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
        "    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
        "\n",
        "    # Convert to binary relevance (1 if rating >= threshold, else 0)\n",
        "    true_binary = (true_ratings >= threshold).astype(int)\n",
        "    predicted_binary = (predicted_ratings >= threshold).astype(int)\n",
        "\n",
        "    precision = precision_score(true_binary, predicted_binary, average='micro')\n",
        "    recall = recall_score(true_binary, predicted_binary, average='micro')\n",
        "\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "import arviz as az\n",
        "import datetime\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "\n",
        "\n",
        "search_space = [\n",
        "    Real(0.1, 3.0, name=\"sigma_u\"),\n",
        "    Real(0.1, 3.0, name=\"sigma_b\"),\n",
        "    Real(0.1, 5.0, name=\"alpha_mu\"),\n",
        "    Real(0.1, 5.0, name=\"beta_mu\")\n",
        "]\n",
        "\n",
        "\n",
        "def objective(params):\n",
        "    sigma_u_val, sigma_b_val, alpha_mu_val, beta_mu_val = params\n",
        "\n",
        "    with pm.Model() as model:\n",
        "        pm.set_data_backend(\"jax\")  # Enables GPU acceleration\n",
        "        pm.set_compute_backend(\"jax\")\n",
        "\n",
        "\n",
        "        mu = pm.Gamma(\"mu\", alpha=alpha_mu_val, beta=beta_mu_val)\n",
        "\n",
        "\n",
        "        user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n",
        "        book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n",
        "\n",
        "\n",
        "        sigma_u = pm.HalfCauchy(\"sigma_u\", beta=sigma_u_val)\n",
        "        sigma_b = pm.HalfCauchy(\"sigma_b\", beta=sigma_b_val)\n",
        "\n",
        "        user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n",
        "        book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n",
        "\n",
        "\n",
        "        lambda_rating = pm.math.exp(\n",
        "            mu +\n",
        "            user_bias[train_user_ids_small] +\n",
        "            book_bias[train_book_ids_small] +\n",
        "            (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\n",
        "        )\n",
        "\n",
        "\n",
        "        ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n",
        "\n",
        "        approx = pm.fit(n=2500, method=\"advi\")\n",
        "        trace = approx.sample(draws=1000)\n",
        "\n",
        "\n",
        "    predicted_ratings = np.exp(\n",
        "        trace.posterior[\"mu\"].mean().item() +\n",
        "        trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] +\n",
        "        trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small] +\n",
        "        (trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] *\n",
        "         trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small]).sum(axis=1)\n",
        "    )\n",
        "\n",
        "\n",
        "    precision = precision_score((test_ratings_small >= 7).astype(int), (predicted_ratings >= 7).astype(int), average='micro')\n",
        "\n",
        "    return -precision  # Minimize negative precision (maximize precision)\n",
        "\n",
        "#bayesian Optimization\n",
        "result = gp_minimize(objective, search_space, n_calls=15, random_state=42, n_jobs=2)\n",
        "\n",
        "# best vals\n",
        "best_sigma_u, best_sigma_b, best_alpha, best_beta = result.x\n",
        "print(f\"Optimal sigma_u: {best_sigma_u}, sigma_b: {best_sigma_b}, alpha_mu: {best_alpha}, beta_mu: {best_beta}\")\n",
        "\n",
        "with pm.Model() as best_model:\n",
        "    mu = pm.Gamma(\"mu\", alpha=best_alpha, beta=best_beta)\n",
        "    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n",
        "    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n",
        "\n",
        "    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=best_sigma_u)\n",
        "    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=best_sigma_b)\n",
        "\n",
        "    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n",
        "    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n",
        "\n",
        "    lambda_rating = pm.math.exp(\n",
        "        mu +\n",
        "        user_bias[train_user_ids_small] +\n",
        "        book_bias[train_book_ids_small] +\n",
        "        (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\n",
        "    )\n",
        "\n",
        "    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n",
        "\n",
        "\n",
        "    approx = pm.fit(n=5000, method=\"advi\")\n",
        "    best_trace = approx.sample(draws=1000)\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model_filename = f\"/content/drive/MyDrive/Colab Notebooks/best_pymc_model_{timestamp}.nc\"\n",
        "az.to_netcdf(best_trace, model_filename)\n",
        "\n",
        "print(f\"Saved best PyMC model to {model_filename}\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "cOt6fTIFKai2",
        "outputId": "576b0174-71a4-4214-c378-480aed952167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nprint(\"Is GPU available?\", torch.cuda.is_available())\\n!pip install pymc[jax] --quiet\\n!pip install jax jaxlib --quiet\\n\\n\\ndef evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\\n    mae = mean_absolute_error(true_ratings, predicted_ratings)\\n    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\\n\\n    # Convert to binary relevance (1 if rating >= threshold, else 0)\\n    true_binary = (true_ratings >= threshold).astype(int)\\n    predicted_binary = (predicted_ratings >= threshold).astype(int)\\n\\n    precision = precision_score(true_binary, predicted_binary, average=\\'micro\\')\\n    recall = recall_score(true_binary, predicted_binary, average=\\'micro\\')\\n\\n    print(f\"MAE: {mae:.4f}\")\\n    print(f\"RMSE: {rmse:.4f}\")\\n    print(f\"Precision: {precision:.4f}\")\\n    print(f\"Recall: {recall:.4f}\")\\n\\n\\n\\nimport arviz as az\\nimport datetime\\nfrom skopt import gp_minimize\\nfrom skopt.space import Real\\n\\n# Define search space for Bayesian Optimization\\nsearch_space = [\\n    Real(0.1, 3.0, name=\"sigma_u\"),\\n    Real(0.1, 3.0, name=\"sigma_b\"),\\n    Real(0.1, 5.0, name=\"alpha_mu\"),\\n    Real(0.1, 5.0, name=\"beta_mu\")\\n]\\n\\n# Objective function (returns precision but doesn\\'t save models)\\ndef objective(params):\\n    sigma_u_val, sigma_b_val, alpha_mu_val, beta_mu_val = params\\n\\n    with pm.Model() as model:\\n        pm.set_data_backend(\"jax\")  # Enables GPU acceleration\\n        pm.set_compute_backend(\"jax\")\\n\\n        # Global mean rating\\n        mu = pm.Gamma(\"mu\", alpha=alpha_mu_val, beta=beta_mu_val)\\n\\n        # User and book biases\\n        user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\\n        book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\\n\\n        # Latent factors\\n        sigma_u = pm.HalfCauchy(\"sigma_u\", beta=sigma_u_val)\\n        sigma_b = pm.HalfCauchy(\"sigma_b\", beta=sigma_b_val)\\n\\n        user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\\n        book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\\n\\n        # Expected ratings\\n        lambda_rating = pm.math.exp(\\n            mu +\\n            user_bias[train_user_ids_small] +\\n            book_bias[train_book_ids_small] +\\n            (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\\n        )\\n\\n        # Poisson likelihood\\n        ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\\n\\n        # Variational inference\\n        approx = pm.fit(n=2500, method=\"advi\")\\n        trace = approx.sample(draws=1000)\\n\\n    # Compute predictions\\n    predicted_ratings = np.exp(\\n        trace.posterior[\"mu\"].mean().item() +\\n        trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] +\\n        trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small] +\\n        (trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] *\\n         trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small]).sum(axis=1)\\n    )\\n\\n    # Evaluate precision\\n    precision = precision_score((test_ratings_small >= 7).astype(int), (predicted_ratings >= 7).astype(int), average=\\'micro\\')\\n\\n    return -precision  # Minimize negative precision (maximize precision)\\n\\n#Bayesian Optimization\\nresult = gp_minimize(objective, search_space, n_calls=15, random_state=42, n_jobs=2)\\n\\n# best vals\\nbest_sigma_u, best_sigma_b, best_alpha, best_beta = result.x\\nprint(f\"Optimal sigma_u: {best_sigma_u}, sigma_b: {best_sigma_b}, alpha_mu: {best_alpha}, beta_mu: {best_beta}\")\\n\\n# **Retrain final model with best parameters**\\nwith pm.Model() as best_model:\\n    mu = pm.Gamma(\"mu\", alpha=best_alpha, beta=best_beta)\\n    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\\n    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\\n\\n    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=best_sigma_u)\\n    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=best_sigma_b)\\n\\n    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\\n    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\\n\\n    lambda_rating = pm.math.exp(\\n        mu +\\n        user_bias[train_user_ids_small] +\\n        book_bias[train_book_ids_small] +\\n        (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\\n    )\\n\\n    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\\n\\n    # final model\\n    approx = pm.fit(n=5000, method=\"advi\")\\n    best_trace = approx.sample(draws=1000)\\n\\n# **Save the best model**\\ntimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\\nmodel_filename = f\"/content/drive/MyDrive/Colab Notebooks/best_pymc_model_{timestamp}.nc\"\\naz.to_netcdf(best_trace, model_filename)\\n\\nprint(f\"Saved best PyMC model to {model_filename}\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pymc==5.19.1 numpyro jax jaxlib scikit-optimize --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import datetime\n",
        "import torch\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "print(\"Is GPU available?\", torch.cuda.is_available())\n",
        "\n",
        "\n",
        "#df = pd.read_excel(\"book_ratings.xlsx\")\n",
        "\n",
        "\n",
        "df = df[['User-ID', 'ISBN', 'Book-Rating']]\n",
        "df = df[df['Book-Rating'] > 0]  # Remove zero ratings\n",
        "\n",
        "# filter out users with too few reviews\n",
        "user_counts = df['User-ID'].value_counts()\n",
        "valid_users = user_counts[user_counts >= 2].index\n",
        "df = df[df['User-ID'].isin(valid_users)]\n",
        "\n",
        "# encode user-ID and ISBN as categorical indices\n",
        "df['User-Index'] = df['User-ID'].astype(\"category\").cat.codes\n",
        "df['Book-Index'] = df['ISBN'].astype(\"category\").cat.codes\n",
        "\n",
        "# make sure we have a contiguous range for indexing\n",
        "df['User-Index'] = df['User-Index'].astype(\"category\").cat.codes\n",
        "df['Book-Index'] = df['Book-Index'].astype(\"category\").cat.codes\n",
        "\n",
        "# rating counts per user and book\n",
        "user_rating_counts = df.groupby('User-Index')['Book-Rating'].count().values\n",
        "book_rating_counts = df.groupby('Book-Index')['Book-Rating'].count().values\n",
        "\n",
        "### revent division by zero\n",
        "user_rating_counts[user_rating_counts == 0] = 1\n",
        "book_rating_counts[book_rating_counts == 0] = 1\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# convert to np arrays for faster access\n",
        "train_user_ids = train_df['User-Index'].values\n",
        "test_user_ids = test_df['User-Index'].values\n",
        "train_book_ids = train_df['Book-Index'].values\n",
        "test_book_ids = test_df['Book-Index'].values\n",
        "train_ratings = train_df['Book-Rating'].values\n",
        "test_ratings = test_df['Book-Rating'].values\n",
        "\n",
        "# **Update counts after filtering**\n",
        "num_users = df['User-Index'].nunique()\n",
        "num_books = df['Book-Index'].nunique()\n",
        "print(f\"Number of unique users: {num_users}, Number of unique books: {num_books}\")\n",
        "\n",
        "# reducing datasize (hardware limitation)\n",
        "use_full_dataset = False\n",
        "latent_dim = 3 ### hardware limitation\n",
        "\n",
        "if use_full_dataset:\n",
        "    train_user_ids_small, train_book_ids_small, train_ratings_small = train_user_ids, train_book_ids, train_ratings\n",
        "    test_user_ids_small, test_book_ids_small, test_ratings_small = test_user_ids, test_book_ids, test_ratings\n",
        "else:\n",
        "    subset_fraction = 0.01  # 1% of training data\n",
        "    subset_size = int(len(train_df) * subset_fraction)\n",
        "    subset_size = max(1000, min(subset_size, 20000))\n",
        "\n",
        "    train_idx_subset = np.random.choice(len(train_user_ids), subset_size, replace=False)\n",
        "    test_idx_subset = np.random.choice(len(test_user_ids), min(subset_size // 2, len(test_user_ids)), replace=False)\n",
        "\n",
        "    train_user_ids_small, train_book_ids_small, train_ratings_small = (\n",
        "        train_user_ids[train_idx_subset],\n",
        "        train_book_ids[train_idx_subset],\n",
        "        train_ratings[train_idx_subset],\n",
        "    )\n",
        "\n",
        "    test_user_ids_small, test_book_ids_small, test_ratings_small = (\n",
        "        test_user_ids[test_idx_subset],\n",
        "        test_book_ids[test_idx_subset],\n",
        "        test_ratings[test_idx_subset],\n",
        "    )\n",
        "\n",
        "    print(f\"Using {subset_size} training rows and {len(test_user_ids_small)} test rows.\")\n",
        "\n",
        "#### evaluate our preds\n",
        "def evaluate_predictions(true_ratings, predicted_ratings, threshold=7):\n",
        "    mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
        "    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
        "    true_binary = (true_ratings >= threshold).astype(int)\n",
        "    predicted_binary = (predicted_ratings >= threshold).astype(int)\n",
        "    precision = precision_score(true_binary, predicted_binary, average='micro')\n",
        "    recall = recall_score(true_binary, predicted_binary, average='micro')\n",
        "    print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "# limited bayesian optimization search space --> limited by hardware\n",
        "search_space = [\n",
        "    Real(0.1, 3.0, name=\"sigma_u\"),\n",
        "    Real(0.1, 3.0, name=\"sigma_b\"),\n",
        "    Real(0.1, 5.0, name=\"alpha_mu\"),\n",
        "    Real(0.1, 5.0, name=\"beta_mu\"),\n",
        "]\n",
        "\n",
        "# objective function for optimization\n",
        "def objective(params):\n",
        "    sigma_u_val, sigma_b_val, alpha_mu_val, beta_mu_val = params\n",
        "    with pm.Model() as model:\n",
        "        mu = pm.Gamma(\"mu\", alpha=alpha_mu_val, beta=beta_mu_val)\n",
        "        user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n",
        "        book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n",
        "        sigma_u = pm.HalfCauchy(\"sigma_u\", beta=sigma_u_val)\n",
        "        sigma_b = pm.HalfCauchy(\"sigma_b\", beta=sigma_b_val)\n",
        "        user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n",
        "        book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n",
        "        lambda_rating = pm.math.exp(mu + user_bias[train_user_ids_small] + book_bias[train_book_ids_small] +\n",
        "                                    (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1))\n",
        "        ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n",
        "        approx = pm.fit(n=2500, method=\"advi\")\n",
        "        trace = approx.sample(draws=500)  # reduced draws to minimize RAM usage --- hardware limitation\n",
        "\n",
        "    predicted_ratings = np.exp(trace.posterior[\"mu\"].mean().item() +\n",
        "                               trace.posterior[\"user_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] +\n",
        "                               trace.posterior[\"book_bias\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small] +\n",
        "                               (trace.posterior[\"user_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_user_ids_small] *\n",
        "                                trace.posterior[\"book_factors\"].mean(dim=(\"chain\", \"draw\")).values[test_book_ids_small]).sum(axis=1))\n",
        "\n",
        "    precision = precision_score((test_ratings_small >= 7).astype(int), (predicted_ratings >= 7).astype(int), average='micro')\n",
        "    return -precision\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sivGzWqaFDSM",
        "outputId": "f9ca0432-51ac-41b2-ed5e-7d02f4a496d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/501.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m501.9/501.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.8/360.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIs GPU available? True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-aab5ff30f8ab>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['User-Index'] = df['User-ID'].astype(\"category\").cat.codes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique users: 28868, Number of unique books: 141472\n",
            "Using 2756 training rows and 1378 test rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **Run Bayesian Optimization**\n",
        "#result = gp_minimize(objective, search_space, n_calls=10, random_state=42, n_jobs=1)\n",
        "\n",
        "#print(f\"Optimal parameters: {result.x}\")\n"
      ],
      "metadata": {
        "id": "eOI1xaNF1e1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YyfmKvn90-F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retraining final model\n",
        "\n",
        "best_sigma_u = 1.2595968179742412\n",
        "best_sigma_b = 0.23533042331948476\n",
        "best_alpha = 4.871402042323151\n",
        "best_beta = 1.2405795681084912\n",
        "\n",
        "with pm.Model() as best_model:\n",
        "    mu = pm.Gamma(\"mu\", alpha=best_alpha, beta=best_beta)\n",
        "    user_bias = pm.Normal(\"user_bias\", mu=0, sigma=0.5 / np.sqrt(user_rating_counts + 1), shape=num_users)\n",
        "    book_bias = pm.Normal(\"book_bias\", mu=0, sigma=0.5 / np.sqrt(book_rating_counts + 1), shape=num_books)\n",
        "\n",
        "    sigma_u = pm.HalfCauchy(\"sigma_u\", beta=best_sigma_u)\n",
        "    sigma_b = pm.HalfCauchy(\"sigma_b\", beta=best_sigma_b)\n",
        "\n",
        "    user_factors = pm.Normal(\"user_factors\", mu=0, sigma=sigma_u, shape=(num_users, latent_dim))\n",
        "    book_factors = pm.Normal(\"book_factors\", mu=0, sigma=sigma_b, shape=(num_books, latent_dim))\n",
        "\n",
        "    lambda_rating = pm.math.exp(\n",
        "        mu + user_bias[train_user_ids_small] + book_bias[train_book_ids_small] +\n",
        "        (user_factors[train_user_ids_small] * book_factors[train_book_ids_small]).sum(axis=1)\n",
        "    )\n",
        "\n",
        "    ratings_obs = pm.Poisson(\"ratings_obs\", mu=lambda_rating, observed=train_ratings_small)\n",
        "\n",
        "    # using jax to accelarate the sampling (numpyro) and utilize multiprocessing\n",
        "    best_trace = pm.sample(\n",
        "        draws=500, tune=500, chains=2,\n",
        "        nuts_sampler=\"numpyro\",\n",
        "        nuts_sampler_kwargs={\"chain_method\": \"vectorized\"}\n",
        "    )\n",
        "\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model_filename = f\"best_pymc_model_{timestamp}.nc\"\n",
        "az.to_netcdf(best_trace, model_filename)\n",
        "\n",
        "print(f\"Saved best PyMC model to {model_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8Szt0Z0RDyh",
        "outputId": "48e091bd-89ab-407d-e58b-498c379d9be3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sample: 100%|██████████| 1000/1000 [48:29<00:00,  2.91s/it]\n",
            "ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best PyMC model to best_pymc_model_2025-03-08_05-38-46.nc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = f\"/content/drive/MyDrive/Colab Notebooks/best_pymc_model_{timestamp}.nc\"\n",
        "az.to_netcdf(best_trace, model_filename)"
      ],
      "metadata": {
        "id": "YTZHYhVFRp3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2bc6cde-e8a7-47de-e08f-096427684726"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/best_pymc_model_2025-03-08_05-38-46.nc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "filename = f\"/content/drive/MyDrive/beta_alpha_sigma_{timestamp}.txt\"\n",
        "with open(filename, \"w\") as f:\n",
        "    f.write(f\"Optimal sigma_u: {best_sigma_u}\\n\")\n",
        "    f.write(f\"Optimal sigma_b: {best_sigma_b}\\n\")\n",
        "    f.write(f\"Optimal alpha_mu: {best_alpha}\\n\")\n",
        "    f.write(f\"Optimal beta_mu: {best_beta}\\n\")\n",
        "\n",
        "print(f\"Saved parameters to {filename}\")\n"
      ],
      "metadata": {
        "id": "UIzRD97rKc0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e43b6f1-ee57-4263-99ca-72af488e4d08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved parameters to /content/drive/MyDrive/beta_alpha_sigma_2025-03-08_05-47-40.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "best_sigma_u, best_sigma_b, best_alpha, best_beta = result.x\n",
        "print(f\"Optimal sigma_u: {best_sigma_u}, sigma_b: {best_sigma_b}, alpha_mu: {best_alpha}, beta_mu: {best_beta}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "djtb5WQ-KiJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "22eh_Fj_Kh3D"
      }
    }
  ]
}